{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL Table Extractor using DSPy\n",
        "\n",
        "This notebook demonstrates how to use DSPy to extract tables from SQL statements in Databricks query history.\n",
        "It handles complex SQL including:\n",
        "- Common Table Expressions (CTEs)\n",
        "- Subqueries\n",
        "- Joins\n",
        "- Nested queries\n",
        "- Temporary tables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install dspy-ai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restart Python kernel to use newly installed packages\n",
        "# dbutils.library.restartPython()  # Uncomment if running in Databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "from typing import List\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure DSPy with Databricks LM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Databricks Foundation Model\n",
        "# You can choose from available models:\n",
        "# - databricks-meta-llama-3-3-70b-instruct (recommended for complex tasks)\n",
        "# - databricks-meta-llama-3-1-70b-instruct\n",
        "# - databricks-dbrx-instruct\n",
        "# - databricks-claude-opus-4-5 (most powerful, use for complex queries)\n",
        "\n",
        "LLM_MODEL_NAME = \"databricks-claude-opus-4-5\"\n",
        "lm = dspy.LM(model=f\"databricks/{LLM_MODEL_NAME}\", cache=False)\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "print(f\"âœ“ Configured DSPy with model: {LLM_MODEL_NAME}\")\n",
        "print(f\"âœ“ Model endpoint: databricks/{LLM_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define DSPy Signatures for Table Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SQLTableExtractor(dspy.Signature):\n",
        "    \"\"\"Extract all table names and column names with detailed usage metadata from a SQL statement.\n",
        "    \n",
        "    Tables include:\n",
        "    - Base tables in FROM clauses\n",
        "    - Tables in JOIN clauses\n",
        "    - Tables in subqueries\n",
        "    - CTE (Common Table Expression) source tables\n",
        "    - Tables in nested queries\n",
        "    \n",
        "    Columns are categorized by usage:\n",
        "    - SELECT columns (output columns, aggregations)\n",
        "    - JOIN columns (columns in ON/USING clauses)\n",
        "    - FILTER columns (columns in WHERE/HAVING conditions)\n",
        "    - GROUP BY columns\n",
        "    - ORDER BY columns\n",
        "    \n",
        "    Important: \n",
        "    - Exclude CTE names themselves (they are temporary)\n",
        "    - Include fully qualified names when present (catalog.schema.table, table.column)\n",
        "    - Return unique names only per category\n",
        "    - For columns, include table prefix if present (e.g., \"customers.customer_id\")\n",
        "    - A column may appear in multiple categories\n",
        "    \"\"\"\n",
        "    \n",
        "    sql_statement: str = dspy.InputField(desc=\"The SQL statement to analyze\")\n",
        "    tables: List[str] = dspy.OutputField(desc=\"List of unique table names referenced in the SQL, excluding CTE names. Include full qualification (catalog.schema.table) when present.\")\n",
        "    columns: List[str] = dspy.OutputField(desc=\"List of ALL unique column names referenced anywhere in the SQL. Include table prefix when present (e.g., 'table.column').\")\n",
        "    join_columns: List[str] = dspy.OutputField(desc=\"List of columns used in JOIN conditions (ON/USING clauses). Include table prefix (e.g., 'table.column').\")\n",
        "    filter_columns: List[str] = dspy.OutputField(desc=\"List of columns used in WHERE and HAVING filter conditions. Include table prefix (e.g., 'table.column').\")\n",
        "    select_columns: List[str] = dspy.OutputField(desc=\"List of columns in SELECT clause, including those in aggregate functions. Include table prefix (e.g., 'table.column').\")\n",
        "    groupby_columns: List[str] = dspy.OutputField(desc=\"List of columns in GROUP BY clause. Include table prefix (e.g., 'table.column').\")\n",
        "    orderby_columns: List[str] = dspy.OutputField(desc=\"List of columns in ORDER BY clause. Include table prefix (e.g., 'table.column').\")\n",
        "    cte_names: List[str] = dspy.OutputField(desc=\"List of CTE (Common Table Expression) names defined in the query (these are temporary, not actual tables)\")\n",
        "    explanation: str = dspy.OutputField(desc=\"Brief explanation of how tables and columns were identified and categorized in this query\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create DSPy Module for Table Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TableExtractorModule(dspy.Module):\n",
        "    \"\"\"Module to extract tables from SQL statements using Chain of Thought reasoning.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.extract = dspy.ChainOfThought(SQLTableExtractor)\n",
        "    \n",
        "    def forward(self, sql_statement: str):\n",
        "        result = self.extract(sql_statement=sql_statement)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Databricks Query History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "SELECT \n",
        "    statement_id,\n",
        "    statement_text,\n",
        "    executed_by_user_id,\n",
        "    start_time,\n",
        "    end_time,\n",
        "    statement_type\n",
        "FROM system.query.history\n",
        "WHERE \n",
        "    statement_type IN ('SELECT', 'INSERT', 'UPDATE', 'DELETE', 'MERGE', 'CREATE_TABLE_AS_SELECT')\n",
        "    AND statement_text IS NOT NULL\n",
        "    AND LENGTH(statement_text) > 50\n",
        "    AND start_time >= CURRENT_DATE - INTERVAL '1' DAYS\n",
        "ORDER BY start_time DESC\n",
        "LIMIT 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store Query Results in a DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the query results from the previous cell\n",
        "query_history_df = _sqldf  # _sqldf contains the last SQL query result\n",
        "print(f\"Retrieved {query_history_df.count()} queries from history\")\n",
        "display(query_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the table extractor module\n",
        "table_extractor = TableExtractorModule()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Sample Complex SQL Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with complex SQL query...\\n\n",
            "================================================================================\n",
            "EXTRACTION RESULTS\n",
            "================================================================================\n",
            "\\nActual Tables Used (5):\n",
            "  - catalog.sales.customers\n",
            "  - catalog.sales.orders\n",
            "  - catalog.sales.order_details\n",
            "  - catalog.products.product_catalog\n",
            "  - catalog.products.featured_products\n",
            "\\nAll Columns Referenced (18):\n",
            "  - c.customer_id\n",
            "  - c.customer_name\n",
            "  - o.order_id\n",
            "  - o.order_date\n",
            "  - o.customer_id\n",
            "  - od.order_id\n",
            "  - od.product_id\n",
            "  - p.product_name\n",
            "  - od.quantity\n",
            "  - od.unit_price\n",
            "  - p.product_id\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "  - od.total_amount\n",
            "  - co.order_id\n",
            "  - product_name\n",
            "  - featured_date\n",
            "  - total_spent\n",
            "\\n JOIN Columns (6):\n",
            "  - c.customer_id\n",
            "  - o.customer_id\n",
            "  - od.product_id\n",
            "  - p.product_id\n",
            "  - co.order_id\n",
            "  - od.order_id\n",
            "\\n FILTER Columns (WHERE/HAVING) (4):\n",
            "  - o.order_date\n",
            "  - od.product_name\n",
            "  - product_name\n",
            "  - featured_date\n",
            "\\n SELECT Columns (14):\n",
            "  - c.customer_id\n",
            "  - c.customer_name\n",
            "  - o.order_id\n",
            "  - o.order_date\n",
            "  - od.order_id\n",
            "  - od.product_id\n",
            "  - p.product_name\n",
            "  - od.quantity\n",
            "  - od.unit_price\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "  - od.total_amount\n",
            "  - product_name\n",
            "  - total_spent\n",
            "\\n GROUP BY Columns (2):\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "\\n ORDER BY Columns (1):\n",
            "  - total_spent\n",
            "\\nCTE Names (temporary) (2):\n",
            "  - customer_orders\n",
            "  - order_details\n",
            "\\nExplanation:\n",
            "  This query uses two CTEs (customer_orders and order_details) which are temporary result sets, not actual tables. The actual tables are: catalog.sales.customers, catalog.sales.orders, catalog.sales.order_details, catalog.products.product_catalog, and catalog.products.featured_products (from the subquery). Columns are categorized by their usage context - JOIN columns appear in ON clauses, FILTER columns in WHERE/HAVING clauses, SELECT columns in output expressions including aggregations, GROUP BY columns for grouping, and ORDER BY for sorting. The computed column total_amount is created in the CTE and total_spent is an alias for the SUM aggregation.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with a complex SQL example\n",
        "test_sql = \"\"\"\n",
        "WITH customer_orders AS (\n",
        "    SELECT \n",
        "        c.customer_id,\n",
        "        c.customer_name,\n",
        "        o.order_id,\n",
        "        o.order_date\n",
        "    FROM catalog.sales.customers c\n",
        "    JOIN catalog.sales.orders o ON c.customer_id = o.customer_id\n",
        "    WHERE o.order_date >= '2024-01-01'\n",
        "),\n",
        "order_details AS (\n",
        "    SELECT \n",
        "        od.order_id,\n",
        "        od.product_id,\n",
        "        p.product_name,\n",
        "        od.quantity * od.unit_price as total_amount\n",
        "    FROM catalog.sales.order_details od\n",
        "    JOIN catalog.products.product_catalog p ON od.product_id = p.product_id\n",
        ")\n",
        "SELECT \n",
        "    co.customer_name,\n",
        "    od.product_name,\n",
        "    SUM(od.total_amount) as total_spent\n",
        "FROM customer_orders co\n",
        "JOIN order_details od ON co.order_id = od.order_id\n",
        "WHERE od.product_name IN (\n",
        "    SELECT product_name \n",
        "    FROM catalog.products.featured_products\n",
        "    WHERE featured_date >= '2024-01-01'\n",
        ")\n",
        "GROUP BY co.customer_name, od.product_name\n",
        "ORDER BY total_spent DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testing with complex SQL query...\\\\n\")\n",
        "result = table_extractor(sql_statement=test_sql)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\\\nActual Tables Used ({len(result.tables)}):\")\n",
        "for table in result.tables:\n",
        "    print(f\"  - {table}\")\n",
        "\n",
        "print(f\"\\\\nAll Columns Referenced ({len(result.columns)}):\")\n",
        "for column in result.columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\n JOIN Columns ({len(result.join_columns)}):\")\n",
        "for column in result.join_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\n FILTER Columns (WHERE/HAVING) ({len(result.filter_columns)}):\")\n",
        "for column in result.filter_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\n SELECT Columns ({len(result.select_columns)}):\")\n",
        "for column in result.select_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\n GROUP BY Columns ({len(result.groupby_columns)}):\")\n",
        "for column in result.groupby_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\n ORDER BY Columns ({len(result.orderby_columns)}):\")\n",
        "for column in result.orderby_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nCTE Names (temporary) ({len(result.cte_names)}):\")\n",
        "for cte in result.cte_names:\n",
        "    print(f\"  - {cte}\")\n",
        "\n",
        "print(f\"\\\\nExplanation:\")\n",
        "print(f\"  {result.explanation}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each query from history\n",
        "results = []\n",
        "\n",
        "for row in query_history_df.collect():\n",
        "    statement_id = row['statement_id']\n",
        "    statement_text = row['statement_text']\n",
        "    executed_by_user_id = row['executed_by_user_id']\n",
        "    \n",
        "    print(f\"\\\\nProcessing query: {statement_id}\")\n",
        "    print(f\"User: {executed_by_user_id}\")\n",
        "    print(f\"SQL Preview: {statement_text[:100]}...\")\n",
        "    \n",
        "    try:\n",
        "        # Extract tables using DSPy\n",
        "        extraction = table_extractor(sql_statement=statement_text)\n",
        "        \n",
        "        result_entry = {\n",
        "            'statement_id': statement_id,\n",
        "            'executed_by_user_id': executed_by_user_id,\n",
        "            'tables': extraction.tables,\n",
        "            'columns': extraction.columns,\n",
        "            'join_columns': extraction.join_columns,\n",
        "            'filter_columns': extraction.filter_columns,\n",
        "            'select_columns': extraction.select_columns,\n",
        "            'groupby_columns': extraction.groupby_columns,\n",
        "            'orderby_columns': extraction.orderby_columns,\n",
        "            'cte_names': extraction.cte_names,\n",
        "            'explanation': extraction.explanation,\n",
        "            'statement_preview': statement_text[:200]\n",
        "        }\n",
        "        results.append(result_entry)\n",
        "        \n",
        "        print(f\"  âœ“ Found {len(extraction.tables)} tables, {len(extraction.columns)} columns\")\n",
        "        print(f\"    - {len(extraction.join_columns)} join cols, {len(extraction.filter_columns)} filter cols\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âœ— Error processing query: {str(e)}\")\n",
        "        results.append({\n",
        "            'statement_id': statement_id,\n",
        "            'executed_by_user_id': executed_by_user_id,\n",
        "            'tables': [],\n",
        "            'columns': [],\n",
        "            'join_columns': [],\n",
        "            'filter_columns': [],\n",
        "            'select_columns': [],\n",
        "            'groupby_columns': [],\n",
        "            'orderby_columns': [],\n",
        "            'cte_names': [],\n",
        "            'explanation': f\"Error: {str(e)}\",\n",
        "            'statement_preview': statement_text[:200]\n",
        "        })\n",
        "\n",
        "print(f\"\\\\n\\\\nProcessed {len(results)} queries successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'statement_id': 'a03cf69c-081e-4766-bd9a-7ddb848c766c',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on the `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.\",\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'},\n",
              " {'statement_id': '63de4f2e-0b0b-4de0-90b2-a085c88c95f9',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': 'This is a Spark SQL query embedded in Python code. The query selects from a table named \"columns\" with a parameterized catalog and schema (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause retrieves `column_name` (wrapped in a lower() function and aliased as \\'n\\') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.',\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'},\n",
              " {'statement_id': 'deedd4c1-446f-48fa-bbf6-b20d33540c60',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.\",\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'},\n",
              " {'statement_id': '39a8d5f6-d593-49cd-8a28-043494d3d907',\n",
              "  'executed_by_user_id': '6102504452920701',\n",
              "  'tables': ['information_schema.table_tags'],\n",
              "  'columns': ['catalog_name', 'schema_name', 'table_name', 'tag_name'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['catalog_name', 'schema_name', 'table_name'],\n",
              "  'select_columns': ['table_name', 'tag_name'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': 'This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.',\n",
              "  'statement_preview': 'tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter'},\n",
              " {'statement_id': 'd1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.\",\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'},\n",
              " {'statement_id': '81ab9b9d-d789-448d-a604-546caae8aff5',\n",
              "  'executed_by_user_id': '6102504452920701',\n",
              "  'tables': [],\n",
              "  'columns': [],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': [],\n",
              "  'select_columns': [],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"The input is not a SQL statement - it is Python code using PySpark's catalog API. The code `existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}` creates a Python set of column names by calling the Spark catalog's listColumns method. Since this is not SQL, no tables or columns can be extracted from a SQL parsing perspective. The reference to `self.logging_table_full_name` is a Python variable, not a SQL table reference.\",\n",
              "  'statement_preview': 'existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}'},\n",
              " {'statement_id': '96b12828-899b-46a8-9c1a-e29c7dd36cb8',\n",
              "  'executed_by_user_id': '3021682792699784',\n",
              "  'tables': ['information_schema.table_tags'],\n",
              "  'columns': ['catalog_name', 'schema_name', 'table_name', 'tag_name'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['catalog_name', 'schema_name', 'table_name'],\n",
              "  'select_columns': ['table_name', 'tag_name'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': 'This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three columns are used in filter conditions (equivalent to WHERE clause): `catalog_name`, `schema_name`, and `table_name`. Two columns are selected in the output: `table_name` and `tag_name`. The `.distinct()` operation is equivalent to SELECT DISTINCT. There are no JOINs, GROUP BY, or ORDER BY operations in this code.',\n",
              "  'statement_preview': 'tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter'},\n",
              " {'statement_id': '7e5f0659-9794-43c9-a7a6-4c5f24041637',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.\",\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'},\n",
              " {'statement_id': '4658bb80-db95-425c-83f7-4801b67c9b87',\n",
              "  'executed_by_user_id': '3021682792699784',\n",
              "  'tables': [],\n",
              "  'columns': [],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': [],\n",
              "  'select_columns': [],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"The input is not a SQL statement - it is Python code using PySpark's catalog API (Context.spark.catalog.listColumns). This code creates a set of column names from a table referenced by the variable `self.logging_table_full_name`. Since this is not SQL, there are no SQL tables, columns, joins, filters, or other SQL constructs to extract. The actual table name would be contained in the Python variable `self.logging_table_full_name` at runtime, but that information is not available from static analysis of this code snippet.\",\n",
              "  'statement_preview': 'existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}'},\n",
              " {'statement_id': '9ec3f6da-8a37-44ab-877c-345c7920e278',\n",
              "  'executed_by_user_id': '7056949509210167',\n",
              "  'tables': ['`{cfg.catalog}`.`{cfg.schema}`.columns'],\n",
              "  'columns': ['column_name', 'data_type', 'asset_urn'],\n",
              "  'join_columns': [],\n",
              "  'filter_columns': ['asset_urn'],\n",
              "  'select_columns': ['column_name', 'data_type'],\n",
              "  'groupby_columns': [],\n",
              "  'orderby_columns': [],\n",
              "  'cte_names': [],\n",
              "  'explanation': \"This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn`. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.\",\n",
              "  'statement_preview': 'rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = \\'{urn.replace(\"\\'\",\"\\'\\'\")}\\'\"\\n).collect()'}]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample - Aggregate Table Usage Statistics - Export to Delta for Further Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "TABLE USAGE STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Total unique tables referenced: 2\n",
            "Total table references: 8\n",
            "\n",
            "Top 10 Most Referenced Tables:\n",
            "--------------------------------------------------------------------------------\n",
            "    6x  `{cfg.catalog}`.`{cfg.schema}`.columns\n",
            "    2x  information_schema.table_tags\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COLUMN USAGE STATISTICS - ALL COLUMNS\n",
            "================================================================================\n",
            "\n",
            "Total unique columns referenced: 7\n",
            "\n",
            "Total column references: 26\n",
            "\n",
            "Top 20 Most Referenced Columns:\n",
            "--------------------------------------------------------------------------------\n",
            "    6x  column_name\n",
            "    6x  data_type\n",
            "    6x  asset_urn\n",
            "    2x  catalog_name\n",
            "    2x  schema_name\n",
            "    2x  table_name\n",
            "    2x  tag_name\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            " JOIN COLUMN USAGE STATISTICS\n",
            "================================================================================\n",
            "\n",
            "Total unique join columns: 0\n",
            "\n",
            "Total join column references: 0\n",
            "\n",
            "Top 10 Most Used Join Columns:\n",
            "--------------------------------------------------------------------------------\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            " FILTER COLUMN USAGE STATISTICS (WHERE/HAVING)\n",
            "================================================================================\n",
            "\n",
            "Total unique filter columns: 4\n",
            "\n",
            "Total filter column references: 12\n",
            "\n",
            "Top 10 Most Used Filter Columns:\n",
            "--------------------------------------------------------------------------------\n",
            "    6x  asset_urn\n",
            "    2x  catalog_name\n",
            "    2x  schema_name\n",
            "    2x  table_name\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Count table and column usage across all queries\n",
        "from collections import Counter\n",
        "\n",
        "all_tables = []\n",
        "all_columns = []\n",
        "all_join_columns = []\n",
        "all_filter_columns = []\n",
        "all_select_columns = []\n",
        "all_groupby_columns = []\n",
        "all_orderby_columns = []\n",
        "\n",
        "for result in results:\n",
        "    all_tables.extend(result['tables'])\n",
        "    all_columns.extend(result['columns'])\n",
        "    all_join_columns.extend(result['join_columns'])\n",
        "    all_filter_columns.extend(result['filter_columns'])\n",
        "    all_select_columns.extend(result['select_columns'])\n",
        "    all_groupby_columns.extend(result['groupby_columns'])\n",
        "    all_orderby_columns.extend(result['orderby_columns'])\n",
        "\n",
        "table_usage = Counter(all_tables)\n",
        "column_usage = Counter(all_columns)\n",
        "join_column_usage = Counter(all_join_columns)\n",
        "filter_column_usage = Counter(all_filter_columns)\n",
        "select_column_usage = Counter(all_select_columns)\n",
        "groupby_column_usage = Counter(all_groupby_columns)\n",
        "orderby_column_usage = Counter(all_orderby_columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TABLE USAGE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique tables referenced: {len(table_usage)}\")\n",
        "print(f\"Total table references: {sum(table_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Referenced Tables:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for table, count in table_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {table}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COLUMN USAGE STATISTICS - ALL COLUMNS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique columns referenced: {len(column_usage)}\")\n",
        "print(f\"\\nTotal column references: {sum(column_usage.values())}\")\n",
        "print(f\"\\nTop 20 Most Referenced Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in column_usage.most_common(20):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" JOIN COLUMN USAGE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique join columns: {len(join_column_usage)}\")\n",
        "print(f\"\\nTotal join column references: {sum(join_column_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Used Join Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in join_column_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" FILTER COLUMN USAGE STATISTICS (WHERE/HAVING)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique filter columns: {len(filter_column_usage)}\")\n",
        "print(f\"\\nTotal filter column references: {sum(filter_column_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Used Filter Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in filter_column_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to Delta Table (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a796892e17d94be5b75b7e9ec0a1960a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nðŸ“Š Created Tables DataFrame with 8 table references\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement_id</th>\n",
              "      <th>executed_by_user_id</th>\n",
              "      <th>table_name</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on the `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a table named \"columns\" with a parameterized catalog and schema (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause retrieves `column_name` (wrapped in a lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>information_schema.table_tags</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>information_schema.table_tags</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three columns are used in filter conditions (equivalent to WHERE clause): `catalog_name`, `schema_name`, and `table_name`. Two columns are selected in the output: `table_name` and `tag_name`. The `.distinct()` operation is equivalent to SELECT DISTINCT. There are no JOINs, GROUP BY, or ORDER BY operations in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7e5f0659-9794-43c9-a7a6-4c5f24041637</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>9ec3f6da-8a37-44ab-877c-345c7920e278</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>`{cfg.catalog}`.`{cfg.schema}`.columns</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn`. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[statement_id: string, executed_by_user_id: string, table_name: string, explanation: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92ad251bd2e74ad59a798ad435e5640f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nðŸ“Š Created Columns DataFrame with 28 column references (categorized by usage)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement_id</th>\n",
              "      <th>executed_by_user_id</th>\n",
              "      <th>column_name</th>\n",
              "      <th>usage_type</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>column_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on the `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>data_type</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on the `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>asset_urn</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a single table with a parameterized fully qualified name (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause references two columns: `column_name` (wrapped in the lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on the `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>column_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a table named \"columns\" with a parameterized catalog and schema (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause retrieves `column_name` (wrapped in a lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>data_type</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a table named \"columns\" with a parameterized catalog and schema (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause retrieves `column_name` (wrapped in a lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>asset_urn</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a table named \"columns\" with a parameterized catalog and schema (`{cfg.catalog}`.`{cfg.schema}`.columns). The SELECT clause retrieves `column_name` (wrapped in a lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>column_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>data_type</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>asset_urn</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>table_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>tag_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>catalog_name</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>schema_name</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>table_name</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three filter conditions are applied on columns `catalog_name`, `schema_name`, and `table_name` (equivalent to WHERE clauses). The select operation retrieves only `table_name` and `tag_name` columns. The `distinct()` call is equivalent to SELECT DISTINCT. No joins, GROUP BY, or ORDER BY operations are present in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>column_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>data_type</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>asset_urn</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is a Spark SQL query embedded in Python code. The query selects from a parameterized table `{cfg.catalog}`.`{cfg.schema}`.columns (a fully qualified table reference where catalog and schema are Python variables). The SELECT clause retrieves `column_name` (transformed with lower() function and aliased as 'n') and `data_type`. The WHERE clause filters on `asset_urn` column. There are no JOINs, GROUP BY, ORDER BY clauses, or CTEs in this simple query.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>table_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three columns are used in filter conditions (equivalent to WHERE clause): `catalog_name`, `schema_name`, and `table_name`. Two columns are selected in the output: `table_name` and `tag_name`. The `.distinct()` operation is equivalent to SELECT DISTINCT. There are no JOINs, GROUP BY, or ORDER BY operations in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>tag_name</td>\n",
              "      <td>SELECT</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three columns are used in filter conditions (equivalent to WHERE clause): `catalog_name`, `schema_name`, and `table_name`. Two columns are selected in the output: `table_name` and `tag_name`. The `.distinct()` operation is equivalent to SELECT DISTINCT. There are no JOINs, GROUP BY, or ORDER BY operations in this code.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>catalog_name</td>\n",
              "      <td>FILTER</td>\n",
              "      <td>This is PySpark DataFrame code rather than raw SQL, but it performs SQL-like operations. The code reads from the `information_schema.table_tags` system table (the catalog name is parameterized). Three columns are used in filter conditions (equivalent to WHERE clause): `catalog_name`, `schema_name`, and `table_name`. Two columns are selected in the output: `table_name` and `tag_name`. The `.distinct()` operation is equivalent to SELECT DISTINCT. There are no JOINs, GROUP BY, or ORDER BY operations in this code.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[statement_id: string, executed_by_user_id: string, column_name: string, usage_type: string, explanation: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nðŸ“ˆ Column Usage Type Distribution:\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0db29d0b6e545d59aeb3cd7447f2f56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+\n",
            "|usage_type|count|\n",
            "+----------+-----+\n",
            "|    SELECT|   16|\n",
            "|    FILTER|   12|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Convert results to Spark DataFrame and save\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Flatten results for DataFrame - separate tables and columns with usage type\n",
        "flattened_table_results = []\n",
        "flattened_column_results = []\n",
        "\n",
        "for result in results:\n",
        "    # Table references\n",
        "    for table in result['tables']:\n",
        "        flattened_table_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            table_name=table,\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    # All columns with usage type\n",
        "    for column in result['select_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='SELECT',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['join_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='JOIN',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['filter_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='FILTER',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['groupby_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='GROUP_BY',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['orderby_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='ORDER_BY',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "\n",
        "# Create and display table references DataFrame\n",
        "if flattened_table_results:\n",
        "    tables_df = spark.createDataFrame(flattened_table_results)\n",
        "    \n",
        "    print(f\"\\\\n Created Tables DataFrame with {tables_df.count()} table references\")\n",
        "    display(tables_df)\n",
        "    \n",
        "    # Optionally save to a Delta table\n",
        "    # tables_df.write.mode(\"overwrite\").saveAsTable(\"your_catalog.your_schema.query_table_analysis\")\n",
        "else:\n",
        "    print(\"No tables extracted from queries.\")\n",
        "\n",
        "# Create and display column references DataFrame with usage type\n",
        "if flattened_column_results:\n",
        "    columns_df = spark.createDataFrame(flattened_column_results)\n",
        "    \n",
        "    print(f\"\\\\n Created Columns DataFrame with {columns_df.count()} column references (categorized by usage)\")\n",
        "    display(columns_df)\n",
        "    \n",
        "    # Show breakdown by usage type\n",
        "    print(\"\\\\n Column Usage Type Distribution:\")\n",
        "    columns_df.groupBy(\"usage_type\").count().orderBy(\"count\", ascending=False).show()\n",
        "    \n",
        "    # Optionally save to a Delta table\n",
        "    # columns_df.write.mode(\"overwrite\").saveAsTable(\"your_catalog.your_schema.query_column_analysis\")\n",
        "else:\n",
        "    print(\"No columns extracted from queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Complex Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using advanced model (Claude Opus) for complex recursive query...\\n\n",
            "Tables extracted:\n",
            "  - hr.employees\n",
            "  - hr.departments\n",
            "\\nðŸ”— Join columns:\n",
            "  - e.manager_id\n",
            "  - oh.employee_id\n",
            "  - emp.employee_id\n",
            "  - d.manager_id\n",
            "\\nðŸ” Filter columns:\n",
            "  - manager_id\n",
            "\\nExplanation: This query uses a recursive CTE `org_hierarchy` to traverse an employee hierarchy. The actual tables are `hr.employees` (referenced twice in the CTE for the base and recursive cases) and `hr.departments` (joined at the outer level). The CTE name `org_hierarchy` is excluded from tables as it's a temporary result set. Columns are categorized as follows: SELECT columns include `employee_id`, `manager_id`, and `level` (with a computed value) from both the base and recursive parts of the CTE. JOIN columns include `e.manager_id` and `oh.employee_id` for the recursive join within the CTE, and `emp.employee_id` and `d.manager_id` for the outer join with departments. The FILTER column is `manager_id` used in the WHERE clause (`manager_id IS NULL`) to find the root of the hierarchy. The `SELECT *` statements select all columns from the CTE and subquery results.\n"
          ]
        }
      ],
      "source": [
        "# For very complex queries, you can use a more powerful model\n",
        "# Example: Using Claude Opus for complex SQL analysis with recursive CTEs\n",
        "\n",
        "complex_sql = \"\"\"\n",
        "SELECT * FROM (\n",
        "    WITH RECURSIVE org_hierarchy AS (\n",
        "        SELECT employee_id, manager_id, 1 as level\n",
        "        FROM hr.employees\n",
        "        WHERE manager_id IS NULL\n",
        "        UNION ALL\n",
        "        SELECT e.employee_id, e.manager_id, oh.level + 1\n",
        "        FROM hr.employees e\n",
        "        JOIN org_hierarchy oh ON e.manager_id = oh.employee_id\n",
        "    )\n",
        "    SELECT * FROM org_hierarchy\n",
        ") emp\n",
        "JOIN hr.departments d ON emp.employee_id = d.manager_id\n",
        "\"\"\"\n",
        "\n",
        "print(\"Using advanced model (Claude Opus) for complex recursive query...\\\\n\")\n",
        "\n",
        "with dspy.context(lm=lm):\n",
        "    result = table_extractor(sql_statement=complex_sql)\n",
        "    \n",
        "    print(\"Tables extracted:\")\n",
        "    for table in result.tables:\n",
        "        print(f\"  - {table}\")\n",
        "    \n",
        "    print(f\"\\\\n Join columns:\")\n",
        "    for column in result.join_columns:\n",
        "        print(f\"  - {column}\")\n",
        "    \n",
        "    print(f\"\\\\n Filter columns:\")\n",
        "    for column in result.filter_columns:\n",
        "        print(f\"  - {column}\")\n",
        "    \n",
        "    print(f\"\\\\nExplanation: {result.explanation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results to JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n================================================================================\n",
            " EXPORT SUMMARY\n",
            "================================================================================\n",
            "\\n Results exported to: sql_table_extraction_results.json\n",
            "\\n Analysis Summary:\n",
            "  - Queries analyzed: 10\n",
            "  - Unique tables found: 2\n",
            "  - Unique columns found: 7\n",
            "  - Join columns: 0\n",
            "  - Filter columns: 4\n",
            "  - Select columns: 4\n",
            "  - Group By columns: 0\n",
            "  - Order By columns: 0\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Export results to JSON for further processing\n",
        "output_file = \"sql_table_extraction_results.json\"\n",
        "\n",
        "# with open(output_file, 'w') as f:\n",
        "#     json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\" EXPORT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\\\n Results exported to: {output_file}\")\n",
        "print(f\"\\\\n Analysis Summary:\")\n",
        "print(f\"  - Queries analyzed: {len(results)}\")\n",
        "print(f\"  - Unique tables found: {len(table_usage)}\")\n",
        "print(f\"  - Unique columns found: {len(column_usage)}\")\n",
        "print(f\"  - Join columns: {len(join_column_usage)}\")\n",
        "print(f\"  - Filter columns: {len(filter_column_usage)}\")\n",
        "print(f\"  - Select columns: {len(select_column_usage)}\")\n",
        "print(f\"  - Group By columns: {len(groupby_column_usage)}\")\n",
        "print(f\"  - Order By columns: {len(orderby_column_usage)}\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
