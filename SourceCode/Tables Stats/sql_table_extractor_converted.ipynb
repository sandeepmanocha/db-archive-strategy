{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SQL Table Extractor using DSPy\n",
        "\n",
        "This notebook demonstrates how to use DSPy to extract tables from SQL statements in Databricks query history.\n",
        "It handles complex SQL including:\n",
        "- Common Table Expressions (CTEs)\n",
        "- Subqueries\n",
        "- Joins\n",
        "- Nested queries\n",
        "- Temporary tables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install dspy-ai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restart Python kernel to use newly installed packages\n",
        "# dbutils.library.restartPython()  # Uncomment if running in Databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "from typing import List\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure DSPy with Databricks LM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Configured DSPy with model: databricks-claude-opus-4-5\n",
            "‚úì Model endpoint: databricks/databricks-claude-opus-4-5\n"
          ]
        }
      ],
      "source": [
        "# Initialize Databricks Foundation Model\n",
        "# You can choose from available models:\n",
        "# - databricks-meta-llama-3-3-70b-instruct (recommended for complex tasks)\n",
        "# - databricks-meta-llama-3-1-70b-instruct\n",
        "# - databricks-dbrx-instruct\n",
        "# - databricks-claude-opus-4-5 (most powerful, use for complex queries)\n",
        "\n",
        "LLM_MODEL_NAME = \"databricks-claude-opus-4-5\"\n",
        "lm = dspy.LM(model=f\"databricks/{LLM_MODEL_NAME}\", cache=False)\n",
        "dspy.configure(lm=lm)\n",
        "\n",
        "print(f\"‚úì Configured DSPy with model: {LLM_MODEL_NAME}\")\n",
        "print(f\"‚úì Model endpoint: databricks/{LLM_MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define DSPy Signatures for Table Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SQLTableExtractor(dspy.Signature):\n",
        "    \"\"\"Extract all table names and column names with detailed usage metadata from a SQL statement.\n",
        "    \n",
        "    Tables include:\n",
        "    - Base tables in FROM clauses\n",
        "    - Tables in JOIN clauses\n",
        "    - Tables in subqueries\n",
        "    - CTE (Common Table Expression) source tables\n",
        "    - Tables in nested queries\n",
        "    \n",
        "    Columns are categorized by usage:\n",
        "    - SELECT columns (output columns, aggregations)\n",
        "    - JOIN columns (columns in ON/USING clauses)\n",
        "    - FILTER columns (columns in WHERE/HAVING conditions)\n",
        "    - GROUP BY columns\n",
        "    - ORDER BY columns\n",
        "    \n",
        "    Important: \n",
        "    - Exclude CTE names themselves (they are temporary)\n",
        "    - Include fully qualified names when present (catalog.schema.table, table.column)\n",
        "    - Return unique names only per category\n",
        "    - For columns, include table prefix if present (e.g., \"customers.customer_id\")\n",
        "    - A column may appear in multiple categories\n",
        "    \"\"\"\n",
        "    \n",
        "    sql_statement: str = dspy.InputField(desc=\"The SQL statement to analyze\")\n",
        "    tables: List[str] = dspy.OutputField(desc=\"List of unique table names referenced in the SQL, excluding CTE names. Include full qualification (catalog.schema.table) when present.\")\n",
        "    columns: List[str] = dspy.OutputField(desc=\"List of ALL unique column names referenced anywhere in the SQL. Include table prefix when present (e.g., 'table.column').\")\n",
        "    join_columns: List[str] = dspy.OutputField(desc=\"List of columns used in JOIN conditions (ON/USING clauses). Include table prefix (e.g., 'table.column').\")\n",
        "    filter_columns: List[str] = dspy.OutputField(desc=\"List of columns used in WHERE and HAVING filter conditions. Include table prefix (e.g., 'table.column').\")\n",
        "    select_columns: List[str] = dspy.OutputField(desc=\"List of columns in SELECT clause, including those in aggregate functions. Include table prefix (e.g., 'table.column').\")\n",
        "    groupby_columns: List[str] = dspy.OutputField(desc=\"List of columns in GROUP BY clause. Include table prefix (e.g., 'table.column').\")\n",
        "    orderby_columns: List[str] = dspy.OutputField(desc=\"List of columns in ORDER BY clause. Include table prefix (e.g., 'table.column').\")\n",
        "    cte_names: List[str] = dspy.OutputField(desc=\"List of CTE (Common Table Expression) names defined in the query (these are temporary, not actual tables)\")\n",
        "    explanation: str = dspy.OutputField(desc=\"Brief explanation of how tables and columns were identified and categorized in this query\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create DSPy Module for Table Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TableExtractorModule(dspy.Module):\n",
        "    \"\"\"Module to extract tables from SQL statements using Chain of Thought reasoning.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.extract = dspy.ChainOfThought(SQLTableExtractor)\n",
        "    \n",
        "    def forward(self, sql_statement: str):\n",
        "        result = self.extract(sql_statement=sql_statement)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Databricks Query History\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57940be513d944ceb893306f2274be72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement_id</th>\n",
              "      <th>statement_text</th>\n",
              "      <th>executed_by_user_id</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>statement_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:27.587</td>\n",
              "      <td>2025-12-09 19:52:27.765</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:27.023</td>\n",
              "      <td>2025-12-09 19:52:27.197</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:26.444</td>\n",
              "      <td>2025-12-09 19:52:26.623</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter(F.col(\"table_name\").isin(table_names))\\n    .select(\"table_name\", \"tag_name\")\\n    .distinct()\\n    .collect()\\n)</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>2025-12-09 19:52:26.085</td>\n",
              "      <td>2025-12-09 19:52:26.439</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:25.862</td>\n",
              "      <td>2025-12-09 19:52:26.044</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>81ab9b9d-d789-448d-a604-546caae8aff5</td>\n",
              "      <td>existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>2025-12-09 19:52:25.819</td>\n",
              "      <td>2025-12-09 19:52:25.944</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter(F.col(\"table_name\").isin(table_names))\\n    .select(\"table_name\", \"tag_name\")\\n    .distinct()\\n    .collect()\\n)</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>2025-12-09 19:52:25.353</td>\n",
              "      <td>2025-12-09 19:52:25.704</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7e5f0659-9794-43c9-a7a6-4c5f24041637</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:25.281</td>\n",
              "      <td>2025-12-09 19:52:25.454</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4658bb80-db95-425c-83f7-4801b67c9b87</td>\n",
              "      <td>existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>2025-12-09 19:52:24.955</td>\n",
              "      <td>2025-12-09 19:52:25.181</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9ec3f6da-8a37-44ab-877c-345c7920e278</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:24.650</td>\n",
              "      <td>2025-12-09 19:52:24.856</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[statement_id: string, statement_text: string, executed_by_user_id: string, start_time: timestamp, end_time: timestamp, statement_type: string]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%sql\n",
        "SELECT \n",
        "    statement_id,\n",
        "    statement_text,\n",
        "    executed_by_user_id,\n",
        "    start_time,\n",
        "    end_time,\n",
        "    statement_type\n",
        "FROM system.query.history\n",
        "WHERE \n",
        "    statement_type IN ('SELECT', 'INSERT', 'UPDATE', 'DELETE', 'MERGE', 'CREATE_TABLE_AS_SELECT')\n",
        "    AND statement_text IS NOT NULL\n",
        "    AND LENGTH(statement_text) > 50\n",
        "    AND start_time >= CURRENT_DATE - INTERVAL '1' DAYS\n",
        "ORDER BY start_time DESC\n",
        "LIMIT 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Store Query Results in a DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0cbf056e3e74449a9a4690b7b8d3250",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 10 queries from history\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "890c28eba3d941d5b360b629f69ec58f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statement_id</th>\n",
              "      <th>statement_text</th>\n",
              "      <th>executed_by_user_id</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>statement_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a03cf69c-081e-4766-bd9a-7ddb848c766c</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:27.587</td>\n",
              "      <td>2025-12-09 19:52:27.765</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>63de4f2e-0b0b-4de0-90b2-a085c88c95f9</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:27.023</td>\n",
              "      <td>2025-12-09 19:52:27.197</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>deedd4c1-446f-48fa-bbf6-b20d33540c60</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:26.444</td>\n",
              "      <td>2025-12-09 19:52:26.623</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39a8d5f6-d593-49cd-8a28-043494d3d907</td>\n",
              "      <td>tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter(F.col(\"table_name\").isin(table_names))\\n    .select(\"table_name\", \"tag_name\")\\n    .distinct()\\n    .collect()\\n)</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>2025-12-09 19:52:26.085</td>\n",
              "      <td>2025-12-09 19:52:26.439</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:25.862</td>\n",
              "      <td>2025-12-09 19:52:26.044</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>81ab9b9d-d789-448d-a604-546caae8aff5</td>\n",
              "      <td>existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}</td>\n",
              "      <td>6102504452920701</td>\n",
              "      <td>2025-12-09 19:52:25.819</td>\n",
              "      <td>2025-12-09 19:52:25.944</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>96b12828-899b-46a8-9c1a-e29c7dd36cb8</td>\n",
              "      <td>tags_rows = (\\n    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\\n    .filter(F.col(\"catalog_name\") == catalog_name)\\n    .filter(F.col(\"schema_name\") == schema_name)\\n    .filter(F.col(\"table_name\").isin(table_names))\\n    .select(\"table_name\", \"tag_name\")\\n    .distinct()\\n    .collect()\\n)</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>2025-12-09 19:52:25.353</td>\n",
              "      <td>2025-12-09 19:52:25.704</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7e5f0659-9794-43c9-a7a6-4c5f24041637</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:25.281</td>\n",
              "      <td>2025-12-09 19:52:25.454</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4658bb80-db95-425c-83f7-4801b67c9b87</td>\n",
              "      <td>existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}</td>\n",
              "      <td>3021682792699784</td>\n",
              "      <td>2025-12-09 19:52:24.955</td>\n",
              "      <td>2025-12-09 19:52:25.181</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9ec3f6da-8a37-44ab-877c-345c7920e278</td>\n",
              "      <td>rows = spark.sql(\\n    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}`.columns WHERE asset_urn = '{urn.replace(\"'\",\"''\")}'\"\\n).collect()</td>\n",
              "      <td>7056949509210167</td>\n",
              "      <td>2025-12-09 19:52:24.650</td>\n",
              "      <td>2025-12-09 19:52:24.856</td>\n",
              "      <td>SELECT</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[statement_id: string, statement_text: string, executed_by_user_id: string, start_time: timestamp, end_time: timestamp, statement_type: string]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get the query results from the previous cell\n",
        "query_history_df = _sqldf  # _sqldf contains the last SQL query result\n",
        "print(f\"Retrieved {query_history_df.count()} queries from history\")\n",
        "display(query_history_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the table extractor module\n",
        "table_extractor = TableExtractorModule()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Sample Complex SQL Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:21:08 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with complex SQL query...\\n\n",
            "================================================================================\n",
            "EXTRACTION RESULTS\n",
            "================================================================================\n",
            "\\nActual Tables Used (5):\n",
            "  - catalog.sales.customers\n",
            "  - catalog.sales.orders\n",
            "  - catalog.sales.order_details\n",
            "  - catalog.products.product_catalog\n",
            "  - catalog.products.featured_products\n",
            "\\nAll Columns Referenced (18):\n",
            "  - c.customer_id\n",
            "  - c.customer_name\n",
            "  - o.order_id\n",
            "  - o.order_date\n",
            "  - o.customer_id\n",
            "  - od.order_id\n",
            "  - od.product_id\n",
            "  - p.product_name\n",
            "  - od.quantity\n",
            "  - od.unit_price\n",
            "  - p.product_id\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "  - od.total_amount\n",
            "  - co.order_id\n",
            "  - product_name\n",
            "  - featured_date\n",
            "  - total_spent\n",
            "\\nüîó JOIN Columns (6):\n",
            "  - c.customer_id\n",
            "  - o.customer_id\n",
            "  - od.product_id\n",
            "  - p.product_id\n",
            "  - co.order_id\n",
            "  - od.order_id\n",
            "\\nüîç FILTER Columns (WHERE/HAVING) (4):\n",
            "  - o.order_date\n",
            "  - od.product_name\n",
            "  - product_name\n",
            "  - featured_date\n",
            "\\nüìä SELECT Columns (13):\n",
            "  - c.customer_id\n",
            "  - c.customer_name\n",
            "  - o.order_id\n",
            "  - o.order_date\n",
            "  - od.order_id\n",
            "  - od.product_id\n",
            "  - p.product_name\n",
            "  - od.quantity\n",
            "  - od.unit_price\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "  - od.total_amount\n",
            "  - product_name\n",
            "\\nüì¶ GROUP BY Columns (2):\n",
            "  - co.customer_name\n",
            "  - od.product_name\n",
            "\\nüìà ORDER BY Columns (1):\n",
            "  - total_spent\n",
            "\\nCTE Names (temporary) (2):\n",
            "  - customer_orders\n",
            "  - order_details\n",
            "\\nExplanation:\n",
            "  This query uses two CTEs (customer_orders and order_details) which are temporary result sets, not actual tables. The actual tables are: catalog.sales.customers, catalog.sales.orders, catalog.sales.order_details, catalog.products.product_catalog, and catalog.products.featured_products (from the subquery). Columns are categorized by their usage context - JOIN columns appear in ON clauses, FILTER columns in WHERE/HAVING conditions, SELECT columns in output expressions including aggregations, GROUP BY columns for grouping, and ORDER BY columns for sorting. The alias 'total_spent' is a computed column used in ORDER BY. Note that 'total_amount' is a computed column from the CTE that's used in the final aggregation.\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with a complex SQL example\n",
        "test_sql = \"\"\"\n",
        "WITH customer_orders AS (\n",
        "    SELECT \n",
        "        c.customer_id,\n",
        "        c.customer_name,\n",
        "        o.order_id,\n",
        "        o.order_date\n",
        "    FROM catalog.sales.customers c\n",
        "    JOIN catalog.sales.orders o ON c.customer_id = o.customer_id\n",
        "    WHERE o.order_date >= '2024-01-01'\n",
        "),\n",
        "order_details AS (\n",
        "    SELECT \n",
        "        od.order_id,\n",
        "        od.product_id,\n",
        "        p.product_name,\n",
        "        od.quantity * od.unit_price as total_amount\n",
        "    FROM catalog.sales.order_details od\n",
        "    JOIN catalog.products.product_catalog p ON od.product_id = p.product_id\n",
        ")\n",
        "SELECT \n",
        "    co.customer_name,\n",
        "    od.product_name,\n",
        "    SUM(od.total_amount) as total_spent\n",
        "FROM customer_orders co\n",
        "JOIN order_details od ON co.order_id = od.order_id\n",
        "WHERE od.product_name IN (\n",
        "    SELECT product_name \n",
        "    FROM catalog.products.featured_products\n",
        "    WHERE featured_date >= '2024-01-01'\n",
        ")\n",
        "GROUP BY co.customer_name, od.product_name\n",
        "ORDER BY total_spent DESC\n",
        "\"\"\"\n",
        "\n",
        "print(\"Testing with complex SQL query...\\\\n\")\n",
        "result = table_extractor(sql_statement=test_sql)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\\\nActual Tables Used ({len(result.tables)}):\")\n",
        "for table in result.tables:\n",
        "    print(f\"  - {table}\")\n",
        "\n",
        "print(f\"\\\\nAll Columns Referenced ({len(result.columns)}):\")\n",
        "for column in result.columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nüîó JOIN Columns ({len(result.join_columns)}):\")\n",
        "for column in result.join_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nüîç FILTER Columns (WHERE/HAVING) ({len(result.filter_columns)}):\")\n",
        "for column in result.filter_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nüìä SELECT Columns ({len(result.select_columns)}):\")\n",
        "for column in result.select_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nüì¶ GROUP BY Columns ({len(result.groupby_columns)}):\")\n",
        "for column in result.groupby_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nüìà ORDER BY Columns ({len(result.orderby_columns)}):\")\n",
        "for column in result.orderby_columns:\n",
        "    print(f\"  - {column}\")\n",
        "\n",
        "print(f\"\\\\nCTE Names (temporary) ({len(result.cte_names)}):\")\n",
        "for cte in result.cte_names:\n",
        "    print(f\"  - {cte}\")\n",
        "\n",
        "print(f\"\\\\nExplanation:\")\n",
        "print(f\"  {result.explanation}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7337ce25df564502b3280c6a1a3ed3f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, bar_style='success'), Label(value='')))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:08 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\nProcessing query: a03cf69c-081e-4766-bd9a-7ddb848c766c\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:15 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\nProcessing query: 63de4f2e-0b0b-4de0-90b2-a085c88c95f9\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:23 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\nProcessing query: deedd4c1-446f-48fa-bbf6-b20d33540c60\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:32 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\nProcessing query: 39a8d5f6-d593-49cd-8a28-043494d3d907\n",
            "User: 6102504452920701\n",
            "SQL Preview: tags_rows = (\n",
            "    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\n",
            "    .filter...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:40 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 4 columns\n",
            "    - 0 join cols, 3 filter cols\n",
            "\\nProcessing query: d1a797fe-e9ff-43c0-9ec2-8ebfe9ecad1e\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:47 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\nProcessing query: 81ab9b9d-d789-448d-a604-546caae8aff5\n",
            "User: 6102504452920701\n",
            "SQL Preview: existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:19:53 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 0 tables, 0 columns\n",
            "    - 0 join cols, 0 filter cols\n",
            "\\nProcessing query: 96b12828-899b-46a8-9c1a-e29c7dd36cb8\n",
            "User: 3021682792699784\n",
            "SQL Preview: tags_rows = (\n",
            "    Context.spark.table(f\"`{catalog_name}`.information_schema.table_tags\")\n",
            "    .filter...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:20:01 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 4 columns\n",
            "    - 0 join cols, 3 filter cols\n",
            "\\nProcessing query: 7e5f0659-9794-43c9-a7a6-4c5f24041637\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:20:09 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\nProcessing query: 4658bb80-db95-425c-83f7-4801b67c9b87\n",
            "User: 3021682792699784\n",
            "SQL Preview: existing_cols = {c.name for c in Context.spark.catalog.listColumns(self.logging_table_full_name)}...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/09 14:20:16 WARNING dspy.primitives.module: Calling module.forward(...) on TableExtractorModule directly is discouraged. Please use module(...) instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úì Found 0 tables, 0 columns\n",
            "    - 0 join cols, 0 filter cols\n",
            "\\nProcessing query: 9ec3f6da-8a37-44ab-877c-345c7920e278\n",
            "User: 7056949509210167\n",
            "SQL Preview: rows = spark.sql(\n",
            "    f\"SELECT lower(column_name) AS n, data_type FROM `{cfg.catalog}`.`{cfg.schema}...\n",
            "  ‚úì Found 1 tables, 3 columns\n",
            "    - 0 join cols, 1 filter cols\n",
            "\\n\\nProcessed 10 queries successfully!\n"
          ]
        }
      ],
      "source": [
        "# Process each query from history\n",
        "results = []\n",
        "\n",
        "for row in query_history_df.collect():\n",
        "    statement_id = row['statement_id']\n",
        "    statement_text = row['statement_text']\n",
        "    executed_by_user_id = row['executed_by_user_id']\n",
        "    \n",
        "    print(f\"\\\\nProcessing query: {statement_id}\")\n",
        "    print(f\"User: {executed_by_user_id}\")\n",
        "    print(f\"SQL Preview: {statement_text[:100]}...\")\n",
        "    \n",
        "    try:\n",
        "        # Extract tables using DSPy\n",
        "        extraction = table_extractor(sql_statement=statement_text)\n",
        "        \n",
        "        result_entry = {\n",
        "            'statement_id': statement_id,\n",
        "            'executed_by_user_id': executed_by_user_id,\n",
        "            'tables': extraction.tables,\n",
        "            'columns': extraction.columns,\n",
        "            'join_columns': extraction.join_columns,\n",
        "            'filter_columns': extraction.filter_columns,\n",
        "            'select_columns': extraction.select_columns,\n",
        "            'groupby_columns': extraction.groupby_columns,\n",
        "            'orderby_columns': extraction.orderby_columns,\n",
        "            'cte_names': extraction.cte_names,\n",
        "            'explanation': extraction.explanation,\n",
        "            'statement_preview': statement_text[:200]\n",
        "        }\n",
        "        results.append(result_entry)\n",
        "        \n",
        "        print(f\"  ‚úì Found {len(extraction.tables)} tables, {len(extraction.columns)} columns\")\n",
        "        print(f\"    - {len(extraction.join_columns)} join cols, {len(extraction.filter_columns)} filter cols\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚úó Error processing query: {str(e)}\")\n",
        "        results.append({\n",
        "            'statement_id': statement_id,\n",
        "            'executed_by_user_id': executed_by_user_id,\n",
        "            'tables': [],\n",
        "            'columns': [],\n",
        "            'join_columns': [],\n",
        "            'filter_columns': [],\n",
        "            'select_columns': [],\n",
        "            'groupby_columns': [],\n",
        "            'orderby_columns': [],\n",
        "            'cte_names': [],\n",
        "            'explanation': f\"Error: {str(e)}\",\n",
        "            'statement_preview': statement_text[:200]\n",
        "        })\n",
        "\n",
        "print(f\"\\\\n\\\\nProcessed {len(results)} queries successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregate Table Usage Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count table and column usage across all queries\n",
        "from collections import Counter\n",
        "\n",
        "all_tables = []\n",
        "all_columns = []\n",
        "all_join_columns = []\n",
        "all_filter_columns = []\n",
        "all_select_columns = []\n",
        "all_groupby_columns = []\n",
        "all_orderby_columns = []\n",
        "\n",
        "for result in results:\n",
        "    all_tables.extend(result['tables'])\n",
        "    all_columns.extend(result['columns'])\n",
        "    all_join_columns.extend(result['join_columns'])\n",
        "    all_filter_columns.extend(result['filter_columns'])\n",
        "    all_select_columns.extend(result['select_columns'])\n",
        "    all_groupby_columns.extend(result['groupby_columns'])\n",
        "    all_orderby_columns.extend(result['orderby_columns'])\n",
        "\n",
        "table_usage = Counter(all_tables)\n",
        "column_usage = Counter(all_columns)\n",
        "join_column_usage = Counter(all_join_columns)\n",
        "filter_column_usage = Counter(all_filter_columns)\n",
        "select_column_usage = Counter(all_select_columns)\n",
        "groupby_column_usage = Counter(all_groupby_columns)\n",
        "orderby_column_usage = Counter(all_orderby_columns)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TABLE USAGE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique tables referenced: {len(table_usage)}\")\n",
        "print(f\"Total table references: {sum(table_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Referenced Tables:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for table, count in table_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {table}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COLUMN USAGE STATISTICS - ALL COLUMNS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique columns referenced: {len(column_usage)}\")\n",
        "print(f\"\\nTotal column references: {sum(column_usage.values())}\")\n",
        "print(f\"\\nTop 20 Most Referenced Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in column_usage.most_common(20):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîó JOIN COLUMN USAGE STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique join columns: {len(join_column_usage)}\")\n",
        "print(f\"\\nTotal join column references: {sum(join_column_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Used Join Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in join_column_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üîç FILTER COLUMN USAGE STATISTICS (WHERE/HAVING)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTotal unique filter columns: {len(filter_column_usage)}\")\n",
        "print(f\"\\nTotal filter column references: {sum(filter_column_usage.values())}\")\n",
        "print(f\"\\nTop 10 Most Used Filter Columns:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for column, count in filter_column_usage.most_common(10):\n",
        "    print(f\"  {count:3d}x  {column}\")\n",
        "\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to Delta Table (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert results to Spark DataFrame and save\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Flatten results for DataFrame - separate tables and columns with usage type\n",
        "flattened_table_results = []\n",
        "flattened_column_results = []\n",
        "\n",
        "for result in results:\n",
        "    # Table references\n",
        "    for table in result['tables']:\n",
        "        flattened_table_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            table_name=table,\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    # All columns with usage type\n",
        "    for column in result['select_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='SELECT',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['join_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='JOIN',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['filter_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='FILTER',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['groupby_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='GROUP_BY',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "    \n",
        "    for column in result['orderby_columns']:\n",
        "        flattened_column_results.append(Row(\n",
        "            statement_id=result['statement_id'],\n",
        "            executed_by_user_id=result['executed_by_user_id'],\n",
        "            column_name=column,\n",
        "            usage_type='ORDER_BY',\n",
        "            explanation=result['explanation']\n",
        "        ))\n",
        "\n",
        "# Create and display table references DataFrame\n",
        "if flattened_table_results:\n",
        "    tables_df = spark.createDataFrame(flattened_table_results)\n",
        "    \n",
        "    print(f\"\\\\nüìä Created Tables DataFrame with {tables_df.count()} table references\")\n",
        "    display(tables_df)\n",
        "    \n",
        "    # Optionally save to a Delta table\n",
        "    # tables_df.write.mode(\"overwrite\").saveAsTable(\"your_catalog.your_schema.query_table_analysis\")\n",
        "else:\n",
        "    print(\"No tables extracted from queries.\")\n",
        "\n",
        "# Create and display column references DataFrame with usage type\n",
        "if flattened_column_results:\n",
        "    columns_df = spark.createDataFrame(flattened_column_results)\n",
        "    \n",
        "    print(f\"\\\\nüìä Created Columns DataFrame with {columns_df.count()} column references (categorized by usage)\")\n",
        "    display(columns_df)\n",
        "    \n",
        "    # Show breakdown by usage type\n",
        "    print(\"\\\\nüìà Column Usage Type Distribution:\")\n",
        "    columns_df.groupBy(\"usage_type\").count().orderBy(\"count\", ascending=False).show()\n",
        "    \n",
        "    # Optionally save to a Delta table\n",
        "    # columns_df.write.mode(\"overwrite\").saveAsTable(\"your_catalog.your_schema.query_column_analysis\")\n",
        "else:\n",
        "    print(\"No columns extracted from queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Use Different LM for Complex Queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For very complex queries, you can use a more powerful model\n",
        "# Example: Using Claude Opus for complex SQL analysis with recursive CTEs\n",
        "\n",
        "complex_sql = \"\"\"\n",
        "SELECT * FROM (\n",
        "    WITH RECURSIVE org_hierarchy AS (\n",
        "        SELECT employee_id, manager_id, 1 as level\n",
        "        FROM hr.employees\n",
        "        WHERE manager_id IS NULL\n",
        "        UNION ALL\n",
        "        SELECT e.employee_id, e.manager_id, oh.level + 1\n",
        "        FROM hr.employees e\n",
        "        JOIN org_hierarchy oh ON e.manager_id = oh.employee_id\n",
        "    )\n",
        "    SELECT * FROM org_hierarchy\n",
        ") emp\n",
        "JOIN hr.departments d ON emp.employee_id = d.manager_id\n",
        "\"\"\"\n",
        "\n",
        "print(\"Using advanced model (Claude Opus) for complex recursive query...\\\\n\")\n",
        "\n",
        "# Use a more powerful model for this specific query\n",
        "advanced_lm = dspy.LM('databricks/databricks-claude-opus-4-1', cache=False)\n",
        "\n",
        "with dspy.context(lm=advanced_lm):\n",
        "    result = table_extractor(sql_statement=complex_sql)\n",
        "    \n",
        "    print(\"Tables extracted:\")\n",
        "    for table in result.tables:\n",
        "        print(f\"  - {table}\")\n",
        "    \n",
        "    print(f\"\\\\nüîó Join columns:\")\n",
        "    for column in result.join_columns:\n",
        "        print(f\"  - {column}\")\n",
        "    \n",
        "    print(f\"\\\\nüîç Filter columns:\")\n",
        "    for column in result.filter_columns:\n",
        "        print(f\"  - {column}\")\n",
        "    \n",
        "    print(f\"\\\\nExplanation: {result.explanation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results to JSON\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results to JSON for further processing\n",
        "output_file = \"/dbfs/tmp/sql_table_extraction_results.json\"\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*80)\n",
        "print(\"üì¶ EXPORT SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\\\n‚úì Results exported to: {output_file}\")\n",
        "print(f\"\\\\nüìä Analysis Summary:\")\n",
        "print(f\"  - Queries analyzed: {len(results)}\")\n",
        "print(f\"  - Unique tables found: {len(table_usage)}\")\n",
        "print(f\"  - Unique columns found: {len(column_usage)}\")\n",
        "print(f\"  - Join columns: {len(join_column_usage)}\")\n",
        "print(f\"  - Filter columns: {len(filter_column_usage)}\")\n",
        "print(f\"  - Select columns: {len(select_column_usage)}\")\n",
        "print(f\"  - Group By columns: {len(groupby_column_usage)}\")\n",
        "print(f\"  - Order By columns: {len(orderby_column_usage)}\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
